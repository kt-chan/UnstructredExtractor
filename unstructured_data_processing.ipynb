{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kt-chan/UnstructredExtractor/blob/main/unstructured_data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZbrRMSiG8xF"
      },
      "source": [
        "# Install Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrGiBMnQzCqk",
        "outputId": "6f0d5903-5769-4e47-e326-1c19bc655a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [969 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,116 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,978 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,253 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,400 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,654 kB]\n",
            "Fetched 10.8 MB in 6s (1,760 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-chi-sim tesseract-ocr-chi-tra libreoffice libtesseract-dev libmagic-dev  > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "IzBi-ChGvCXV"
      },
      "outputs": [],
      "source": [
        "!pip install -r ./llms/requirements.txt  > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkrTCcgiEqxh"
      },
      "source": [
        "# Setup MongoDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2zVG69g45wm"
      },
      "source": [
        "Connection Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT3b_EqlE3Dl",
        "outputId": "f52ce4a6-8584-4880-a7a9-98b54ce070ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.106.45.236\n",
            "Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.10/dist-packages (4.8.0)\n",
            "\u001b[33mWARNING: pymongo 4.8.0 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo[srv]) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "# set this to ip to MongoDB atlas firewall rules, https://cloud.mongodb.com/v2/667549db2c13183084c47650#/security/network/accessList\n",
        "!curl ifconfig.me\n",
        "print()\n",
        "!python -m pip install \"pymongo[srv]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBTTxYjiMD42",
        "outputId": "f14c2772-5731-489c-8a1a-afbb462dbebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "user = userdata.get('mongodb_username')  ## replace with mongodb atlas username\n",
        "password = userdata.get('mongodb_password')## replace with mongodb atlas password\n",
        "uri = \"mongodb+srv://\"+user+\":\"+password+\"@ktchan-mongo-atlast.mvx53s5.mongodb.net/?retryWrites=true&w=majority&appName=ktchan-mongo-atlast\"\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGeUSDVtHEhP"
      },
      "source": [
        "# Content Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU9JE0kv8-zW"
      },
      "source": [
        "Parittion the pdf files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEOttQcEva_m",
        "outputId": "bb88172b-96f1-4055-df83-8232d36bec2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download completed. File saved as: /content/files/downloaded_file.pdf\n"
          ]
        }
      ],
      "source": [
        "import os, requests\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import elements_to_json\n",
        "\n",
        "# For this notebook I uploaded Nvidia's earnings into the Files directory called \"/content/\"\n",
        "pdf_url = \"https://static.www.tencent.com/uploads/2024/04/08/e95c902973fc282be3b3e285c6245281.pdf\"\n",
        "output_dir = \"./files/\"\n",
        "\n",
        "def download_pdf(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    full_path = os.path.abspath(output_dir+filename)\n",
        "    with open(full_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f'Download completed. File saved as: {full_path}')\n",
        "    return full_path\n",
        "\n",
        "\n",
        "filename = download_pdf(pdf_url, 'downloaded_file.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB16l73cxbEr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from unstructured.staging.base import elements_to_text\n",
        "from unstructured.cleaners.core import clean_non_ascii_chars\n",
        "from unstructured.cleaners.core import group_broken_paragraphs\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "\n",
        "# Partition PDF\n",
        "# Define parameters for Unstructured's library\n",
        "strategy = \"hi_res\" # Strategy for analyzing PDFs and extracting table structure\n",
        "model_name = \"yolox\" # Best model for table extraction. Other options are detectron2_onnx and chipper depending on file layout\n",
        "\n",
        "# Extracts the elements from the PDF\n",
        "elements = partition_pdf(\n",
        "  filename=filename,\n",
        "  strategy=strategy,\n",
        "  infer_table_structure=True,\n",
        "  model_name=model_name\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Extract tables\n",
        "para_split_re = re.compile(r\"(\\s*\\n\\s*){3}\")\n",
        "element_text = elements_to_text(elements)\n",
        "element_text = group_broken_paragraphs(element_text, paragraph_split=para_split_re)\n",
        "element_text = clean_non_ascii_chars(element_text)\n",
        "\n",
        "# Store results in files\n",
        "elements_to_json(elements, filename=f\"{filename}.json\") # Takes a while for file to show up on the Google Colab\n",
        "elements_to_text(elements, filename=f\"{filename}.text\") # Takes a while for file to show up on the Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ5_Wsrc-YGp"
      },
      "source": [
        "# Table Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mswW8i_FcwHZ",
        "outputId": "c68010d2-d806-4221-c9b8-73510b4e4fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written 158 tables documents into mongodb collection_tencent.\n",
            "Written 3476 element documents into mongodb collection_tencent_notes.\n",
            "Element Tree size is 3477.\n"
          ]
        }
      ],
      "source": [
        "## In order to extract only the table elements I’ve written a helper function to do so:\n",
        "import re\n",
        "import json\n",
        "from html import escape\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from treelib import Node, Tree\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "def extract_json_elements(input_filename, element_tree):\n",
        "  # Read the JSON file\n",
        "  extracted_elements = {}\n",
        "\n",
        "  with open(input_filename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    # Iterate over the JSON data and extract required elements\n",
        "    for entry in data:\n",
        "      text = entry[\"text\"]\n",
        "\n",
        "      if \"Table\" == entry[\"type\"]:\n",
        "          text = entry[\"metadata\"][\"text_as_html\"]\n",
        "\n",
        "      if 'parent_id' in entry['metadata']:\n",
        "        extracted_elements[entry[\"element_id\"]] = {\n",
        "          \"_id\": entry[\"element_id\"],\n",
        "          \"filename\":entry[\"metadata\"][\"filename\"],\n",
        "          \"page_number\":entry[\"metadata\"][\"page_number\"],\n",
        "          \"parent_id\":entry[\"metadata\"][\"parent_id\"],\n",
        "          \"type\":entry[\"type\"],\n",
        "          \"text\":text\n",
        "          }\n",
        "      else:\n",
        "        extracted_elements[entry[\"element_id\"]] = {\n",
        "            \"_id\": entry[\"element_id\"],\n",
        "            \"filename\":entry[\"metadata\"][\"filename\"],\n",
        "            \"page_number\":entry[\"metadata\"][\"page_number\"],\n",
        "            \"type\":entry[\"type\"],\n",
        "            \"text\":text\n",
        "            }\n",
        "\n",
        "  build_dependency(extracted_elements, element_tree)\n",
        "  return extracted_elements\n",
        "\n",
        "def build_dependency(extracted_elements, element_tree):\n",
        "    # construct element dependency tree\n",
        "  for k, element in extracted_elements.items():\n",
        "      if \"parent_id\" in element:\n",
        "        # print(element)\n",
        "        parent_target = element[\"parent_id\"]\n",
        "        node = element_tree.get_node(parent_target)\n",
        "        if node:\n",
        "          element.pop(\"parent_id\", None)\n",
        "          element_tree.create_node(k, k, parent=parent_target, data = element)\n",
        "        else:\n",
        "          element.pop(\"parent_id\", None)\n",
        "          # element_parent = extracted_elements[parent_target].pop(\"parent_id\", None)\n",
        "          element_tree.create_node(parent_target, parent_target, parent=\"root\", data = extracted_elements[parent_target])\n",
        "          element_tree.create_node(k, k, parent=parent_target, data = element)\n",
        "      else:\n",
        "        element_tree.create_node(k, k, parent=\"root\", data = element)\n",
        "\n",
        "\n",
        "def extract_json_table(input_filename):\n",
        "  # Read the JSON file\n",
        "  elements_table = []\n",
        "  with open(input_filename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    # Iterate over the JSON data and extract required table elements\n",
        "    for entry in data:\n",
        "      if entry[\"type\"] == \"Table\":\n",
        "        entry[\"metadata\"][\"element_id\"] = entry[\"element_id\"]\n",
        "        elements_table.append({\"element_id\":entry[\"element_id\"], \"metadata\" : entry[\"metadata\"]})\n",
        "\n",
        "  return parse_html_table(elements_table)\n",
        "\n",
        "\n",
        "#Define a function to clean table columns\n",
        "def extract_table_from_html(table_html_string):\n",
        "  # Rename the columns to be just the first element of each tuple\n",
        "  # Function to convert elements to string\n",
        "  def to_str(element):\n",
        "      if isinstance(element, tuple):\n",
        "          return '_'.join(map(str, element))  # Join tuple elements with an underscore\n",
        "      else:\n",
        "          return str(element)  # Convert integer to string\n",
        "\n",
        "  # Find the table within the span element\n",
        "  table = table_html_string.find('table')\n",
        "  df_flattened = None\n",
        "  if table:\n",
        "      table_dfs = pd.read_html(StringIO(table_html_string)) # Get the first DataFrame\n",
        "      table_df = table_dfs[0] # assume only one table per json tab\n",
        "      df_flattened = table_df.map(str)  # Convert all values to string\n",
        "      df_flattened.columns = [to_str(col) for col in df_flattened.columns] # Column headers parsed as set by pdf extractor, force to string for compatibility\n",
        "\n",
        "  return df_flattened\n",
        "\n",
        "\n",
        "# Define a fuctnion to formatn tables cells\n",
        "def format_table(df_table):\n",
        "    # df_table =  pd.DataFrame(ds_table)\n",
        "    df_numeric = df_table.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
        "    df_combined = pd.concat([df_table.iloc[:, 0], df_numeric], axis=1)\n",
        "    df_cleaned = df_combined.dropna(how='any')\n",
        "    return df_cleaned\n",
        "\n",
        "\n",
        "# Define a function to clean the data\n",
        "def clean_text(text):\n",
        "\n",
        "    # Function to convert matched text to negative number\n",
        "    def make_negative(match):\n",
        "        number = match.group(1).replace(',', '')\n",
        "        return f\"{-int(number)}\"\n",
        "\n",
        "    # Check if the text is a string\n",
        "    if isinstance(text, str):\n",
        "        # Replace unwanted characters if it's a string\n",
        "        text = text.replace('#', '').replace('*', '')\n",
        "\n",
        "\n",
        "        # Regular expression to match numbers in parentheses with commas\n",
        "        pattern = r'\\((\\d{1,3}(,\\d{3})*)\\)'\n",
        "        # Replace all occurrences of the pattern with its negative equivalent\n",
        "        text = re.sub(pattern, make_negative, text)\n",
        "\n",
        "        # Check if the text is a number with commas\n",
        "        if re.match(r'^-?\\d{1,3}(,\\d{3})*\\.\\d+$', text):\n",
        "            # Remove commas and convert to float\n",
        "            return float(text.replace(',', ''))\n",
        "        # Check if the text is an integer with commas\n",
        "        elif re.match(r'^-?\\d{1,3}(,\\d{3})*$', text):\n",
        "            # Remove commas and convert to integer\n",
        "            return int(text.replace(',', ''))\n",
        "    # Return the text as is if it's not a string\n",
        "    return text\n",
        "\n",
        "\n",
        "# Define a function to clean the data\n",
        "def parse_html_table(data_tables):\n",
        "\n",
        "  # Initialize an empty dictionary to hold the tables with span_id as the key\n",
        "  # and the table data as the value\n",
        "  tables_dict = {}\n",
        "\n",
        "\n",
        "  # Find all span elements with an 'id' attribute\n",
        "  for data_table in data_tables:\n",
        "      span_id = data_table['element_id']\n",
        "\n",
        "      # Initialize a dictionary to hold the metadata and table data for the current span\n",
        "      span_data = {}\n",
        "\n",
        "      # Extract the 'metadata' attribute if it exists\n",
        "      metadata = data_table.get('metadata', None)\n",
        "\n",
        "      if metadata:\n",
        "          span_data['metadata'] = metadata\n",
        "\n",
        "      # Find the table within the span element\n",
        "      table = data_table.get('text_as_html', None)\n",
        "      if table:\n",
        "          # Use pandas to read the table\n",
        "          table_df = pd.read_html(StringIO(table)) # Get the first DataFrame\n",
        "          span_data['table'] = table_df\n",
        "\n",
        "      # Add the span data to the main dictionary using the span_id as the key\n",
        "      tables_dict[span_id] = span_data\n",
        "\n",
        "\n",
        "  datasets = []\n",
        "\n",
        "  # Get the first key-value pair based on insertion order\n",
        "  for span_id, span_data in tables_dict.items():\n",
        "      # Access the metadata and the table DataFrame\n",
        "      metadata = span_data.get('metadata')\n",
        "      table_html = metadata.get('text_as_html')\n",
        "      df_table = extract_table_from_html(table_html)\n",
        "      df_table_clean = df_table.map(clean_text)\n",
        "      datasets.append({\n",
        "          \"_id\": span_id,  # Corrected the syntax for dictionary keys (no quotes)\n",
        "          \"meta\": metadata,  # Corrected the variable name ('metadata' instead of 'metdata')\n",
        "          \"data\": df_table_clean.to_dict(\"records\"),  # Assuming you want to store the first DataFrame in the list\n",
        "          \"data_raw\": df_table.to_dict(\"records\")  # raw data format, without formatting\n",
        "          })\n",
        "\n",
        "  return datasets\n",
        "\n",
        "\n",
        "file_path=\"/content/files/downloaded_file.pdf.json\"\n",
        "element_tree = Tree()\n",
        "element_tree.create_node(\"root\", \"root\")\n",
        "elements = extract_json_elements(file_path, element_tree)\n",
        "elements_tables = extract_json_table(file_path)\n",
        "\n",
        "\n",
        "db = client['db_finance_report']\n",
        "collection = db['collection_tencent']\n",
        "collection.drop()\n",
        "insert_collection = elements_tables\n",
        "collection.insert_many(insert_collection);\n",
        "print(f'Written {collection.count_documents({})} tables documents into mongodb collection_tencent.')\n",
        "\n",
        "\n",
        "collection = db['collection_tencent_notes']\n",
        "collection.drop()\n",
        "collection.insert_many(list(elements.values()));\n",
        "print(f'Written {collection.count_documents({})} element documents into mongodb collection_tencent_notes.')\n",
        "\n",
        "\n",
        "print(f'Element Tree size is {element_tree.size()}.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA-Klk_s9BeC"
      },
      "source": [
        "Extract tables from paritions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5-C5KuaBV8W"
      },
      "outputs": [],
      "source": [
        "# print(element_tree)\n",
        "\n",
        "# Depth-First Search (DFS) traversal\n",
        "def dfs_traversal(node: Node, leaf_root: Node, tree: Tree):\n",
        "    if node:\n",
        "      if leaf_root.data and 'text' in leaf_root.data:\n",
        "         if not node.is_root():\n",
        "            prefix = f\"<span id='{leaf_root.identifier}'>\" + node.data[\"text\"] + \"</span>\"\n",
        "            leaf_root.data[\"text\"]  =  prefix + leaf_root.data[\"text\"]\n",
        "            # print(\"</br>\" + leaf_root.data[\"text\"] + \"</br>\")\n",
        "      dfs_traversal(tree.parent(node.identifier), leaf_root, tree)\n",
        "\n",
        "# Start the traversal from the root node\n",
        "# target_node = element_tree.get_node(\"0ce1e2d0bb4281b12e30ad32b5653f36\")\n",
        "for table in elements_tables:\n",
        "  leaf = element_tree.get_node(table[\"_id\"])\n",
        "  if leaf:\n",
        "    parent = element_tree.parent(leaf.identifier)\n",
        "    if(leaf and parent):\n",
        "      dfs_traversal(parent, leaf, element_tree)\n",
        "  else:\n",
        "    print(table)\n",
        "\n",
        "\n",
        "# Function to print all data in the tree\n",
        "def print_all_data(tree, filename):\n",
        "  with open(filename, 'w') as file:  # Open the file in write mode\n",
        "    for node in tree.all_nodes():\n",
        "        # Check if 'text' key exists in the data dictionary\n",
        "        if node.data and 'text' in node.data:\n",
        "            # Print the value associated with 'text' key\n",
        "             file.write(f\"{node.data['text']}\\n\")  #\n",
        "\n",
        "# Call the function to print all data\n",
        "print_all_data(element_tree, \"./files/downloaded_file.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Setup"
      ],
      "metadata": {
        "id": "Fd8vIMWaetjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/llms/zhipuchat.py\n",
        "## This is auto generated from colab scripts.\n",
        "import os\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import httpx\n",
        "from google.colab import userdata\n",
        "\n",
        "class CustomClient(httpx.Client):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Initialize httpx.Client with verify=False to disable SSL verification\n",
        "        super().__init__(verify=False, *args, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ZhipuChat():\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Initialize httpx.Client with verify=False to disable SSL verification\n",
        "        # Load .env file\n",
        "        load_dotenv()\n",
        "        # Initialize the OpenAI client with the custom HTTP client\n",
        "        self._client = OpenAI(\n",
        "            api_key=userdata.get('ZHIPU_API_KEY'),\n",
        "            base_url=\"https://open.bigmodel.cn/api/paas/v4/\",\n",
        "            http_client= CustomClient()  # Use the custom HTTP client\n",
        "        )\n",
        "\n",
        "    def getClient(self):\n",
        "        return self._client\n",
        "\n",
        "    def getCompletion(self, messages):\n",
        "        completion = self.getClient().chat.completions.create(\n",
        "            model=\"glm-4-flash\",\n",
        "            messages=messages,\n",
        "            max_tokens=4095,\n",
        "            top_p=0.7,\n",
        "            temperature=0.9\n",
        "        )\n",
        "        return completion;\n",
        "\n",
        "    def chat_ping(self):\n",
        "        # Check if completion was successful and print the message\n",
        "        completion= self.getCompletion([\n",
        "              {\"role\": \"system\", \"content\": \"你是人工智能助手...\"},\n",
        "              {\"role\": \"user\", \"content\": \"你好，我叫李雷，1+1等于多少？\"}\n",
        "          ])\n",
        "        if completion and completion.choices:\n",
        "            print(completion.choices[0].message.content)\n",
        "\n",
        "    def chat_call(self, content, question):\n",
        "        print(\"content length: \" + str(len(content)))\n",
        "        print(question + \"\\n\\n\")\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"你是文件分析助理，擅长分析文章内容并提取所有关键信息。所有输出以中文输出，要详细，覆盖用户问题的所有内容。\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": content,\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": question,\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # 然后调用 chat-completion, 获取 Kimi 的回答\n",
        "        completion = self.getCompletion(messages)\n",
        "        print(completion.choices[0].message)\n",
        "\n",
        "\n",
        "# agentchat = ZhipuChat()\n",
        "# agentchat.chat_ping()\n",
        "# agentchat.file_call(r'./files/downloaded_file.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NgzEeIrI4P1",
        "outputId": "5477d0d0-2ec9-4a0a-ce20-8046c7d1e35b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/llms/zhipuchat.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGI5BxbXnwgq",
        "outputId": "25308733-e09e-455b-ed9f-765d90a11f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "content length: 700927\n",
            "content length: 700927\n",
            "提取文中主题，要详细覆盖文中所有章节。以Json Array list 输出\n",
            "\n",
            "\n",
            "ChatCompletionMessage(content='出\\n\\n```json\\n[\\n  {\\n    \"title\": \"公司治理\",\\n    \"content\": \"包括董事会结构、独立非执行董事、提名委员会、薪酬委员会、审计委员会、风险管理、内部控制、股东、股息政策等。\",\\n    \"chapters\": [\\n      \"公司治理结构\",\\n      \"董事会\",\\n      \"独立非执行董事\",\\n      \"提名委员会\",\\n      \"薪酬委员会\",\\n      \"审计委员会\",\\n      \"风险管理\",\\n      \"内部控制\",\\n      \"股东\",\\n      \"股息政策\"\\n    ]\\n  },\\n  {\\n    \"title\": \"财务报告\",\\n    \"content\": \"包括财务报表、财务风险、关键会计估计和判断、分部信息、收入、费用、收益、税务、每股收益、员工福利费用、董事的利益、股息等。\",\\n    \"chapters\": [\\n      \"财务报表概述\",\\n      \"财务风险\",\\n      \"关键会计估计和判断\",\\n      \"分部信息\",\\n      \"收入\",\\n      \"费用\",\\n      \"收益\",\\n      \"税务\",\\n      \"每股收益\",\\n      \"员工福利费用\",\\n      \"董事的利益\",\\n      \"股息\"\\n    ]\\n  },\\n  {\\n    \"title\": \"业务\",\\n    \"content\": \"包括增值服务、在线广告、金融科技和商务服务、其他业务等。\",\\n    \"chapters\": [\\n      \"增值服务\",\\n      \"在线广告\",\\n      \"金融科技和商务服务\",\\n      \"其他业务\"\\n    ]\\n  },\\n  {\\n    \"title\": \"风险因素\",\\n    \"content\": \"包括市场竞争和创新风险、宏观经济风险、监管和合规风险、信息安全风险、危机管理、公关和声誉风险、IT系统风险、持续经营风险、投资管理风险、ToB业务风险、社会责任和环境保护风险、欺诈风险等。\",\\n    \"chapters\": [\\n      \"市场竞争和创新风险\",\\n      \"宏观经济风险\",\\n      \"监管和合规风险\",\\n      \"信息安全风险\",\\n      \"危机管理、公关和声誉风险\",\\n      \"IT系统风险\",\\n      \"持续经营风险\",\\n      \"投资管理风险\",\\n      \"ToB业务风险\",\\n      \"社会责任和环境保护风险\",\\n      \"欺诈风险\"\\n    ]\\n  },\\n  {\\n    \"title\": \"可持续发展\",\\n    \"content\": \"包括环境保护、社会责任、员工关怀、社区参与等方面。\",\\n    \"chapters\": [\\n      \"环境保护\",\\n      \"社会责任\",\\n      \"员工关怀\",\\n      \"社区参与\"\\n    ]\\n  }\\n]\\n```', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "# from llms.zhipuchat import ZhipuChat\n",
        "# !find . -type f -name \"*.pyc\" -delete\n",
        "# %load /content/llms/zhipuchat.py\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from llms.zhipuchat import ZhipuChat\n",
        "\n",
        "agentchat = ZhipuChat()\n",
        "filepath = \"/content/files/downloaded_file.txt\"\n",
        "with open(filepath, 'r') as f:\n",
        "  content = f.read()  # Read the entire content of the file\n",
        "  print(\"content length: \" + str(len(content)))\n",
        "  agentchat.chat_call(content,\"提取文中主题，要详细覆盖文中所有章节。以Json Array list输出。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nao00jRUNfdJ"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19Ushdnuovhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VU9tmNY8Jk-"
      },
      "source": [
        "Testing Scripts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello\")"
      ],
      "metadata": {
        "id": "q-6Hs-Nte254"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6csgZ_1PHq"
      },
      "source": [
        "# Save files to Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./files ./llms"
      ],
      "metadata": {
        "id": "K6PckLVP9eoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abwwbC8y0T4V",
        "outputId": "14e0ce30-b939-4b72-ab9e-0435a1e95b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File copied: downloaded_file.pdf.json\n",
            "File copied: requirements.txt\n",
            "File copied: downloaded_file.pdf\n",
            "File copied: downloaded_file.pdf.text\n",
            "File copied: __init__.py\n",
            "File copied: kimichat.py\n",
            "All files copied to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "source_dir = '/content/files'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "target_dir = '/content/drive/MyDrive/Colab Notebooks/files'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "source_dir = '/content/llms'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "target_dir = '/content/drive/MyDrive/Colab Notebooks/llms'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "print('All files copied to Google Drive.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHRAZjHc7T7v",
        "outputId": "324fec24-c0a2-41b9-dd99-a44198a228ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File copied: downloaded_file.txt\n",
            "File copied: downloaded_file.pdf.json\n",
            "File copied: downloaded_file.pdf.text\n",
            "File copied: downloaded_file.pdf\n",
            "File copied: zhipuchat.py\n",
            "File copied: __init__.py\n",
            "File copied: requirements.txt\n",
            "File copied: kimichat.py\n",
            "All files copied to %s. /content/llms\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "target_dir = '/content/files'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/files'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "target_dir = '/content/llms'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/llms'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "print(f'All files copied to %s.', target_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FHYXQv4x6WzvoFYcHkRzzw3I8e3mX7rc",
      "authorship_tag": "ABX9TyMa38OKy5l9VeNhz4pxfT5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}